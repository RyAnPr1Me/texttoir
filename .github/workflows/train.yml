name: Train Text-to-LLVM IR Model

on:
  push:
    branches: [ main, master ]
  pull_request:
    branches: [ main, master ]
  workflow_dispatch:
    inputs:
      num_epochs:
        description: 'Number of training epochs'
        required: false
        default: '10'
      batch_size:
        description: 'Batch size'
        required: false
        default: '8'
      dataset_size:
        description: 'Number of examples to generate (default: 500000 for ~1GB, large dataset for best quality)'
        required: false
        default: '500000'

permissions:
  contents: read

jobs:
  generate-data:
    runs-on: ubuntu-latest
    permissions:
      contents: read
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
      
      - name: Install LLVM tools for IR validation
        run: |
          sudo apt-get update -qq
          sudo apt-get install -y llvm
          llvm-as --version
      
      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install tqdm
      
      - name: Generate training data with validation
        run: |
          python data/generate_data_large.py \
            --target-examples ${{ github.event.inputs.dataset_size || '250000' }} \
            --output-dir dataset \
            --variations-per-example 15 \
            --seed 42
      
      - name: Verify dataset size
        run: |
          echo "Dataset files:"
          ls -lh dataset/
          echo ""
          echo "Total dataset size:"
          du -sh dataset/
          echo ""
          echo "Number of examples per split:"
          wc -l dataset/*.jsonl
      
      - name: Upload dataset
        uses: actions/upload-artifact@v4
        with:
          name: dataset
          path: dataset/
          retention-days: 7

  train-model:
    needs: generate-data
    runs-on: ubuntu-latest
    permissions:
      contents: read
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: Download dataset
        uses: actions/download-artifact@v4
        with:
          name: dataset
          path: dataset/
      
      - name: Verify dataset
        run: |
          echo "Downloaded dataset:"
          ls -lh dataset/
          wc -l dataset/*.jsonl
      
      - name: Train model
        run: |
          python training/train.py \
            --num_epochs ${{ github.event.inputs.num_epochs || '10' }} \
            --batch_size ${{ github.event.inputs.batch_size || '8' }} \
            --gradient_accumulation_steps 4 \
            --output_dir checkpoints \
            --use_amp \
            --use_gradient_checkpointing \
            --num_workers 2 \
            --early_stopping_patience 3
      
      - name: Upload model checkpoint
        uses: actions/upload-artifact@v4
        with:
          name: model-checkpoint
          path: checkpoints/
          retention-days: 30
